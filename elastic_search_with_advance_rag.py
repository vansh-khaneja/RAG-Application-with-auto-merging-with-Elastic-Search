# -*- coding: utf-8 -*-
"""elastic_search_with advance_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10g_5zuh5wjcPFRpELUgp8nZRYO8j1KvU
"""

from llama_index.readers.file import PyMuPDFReader

loader = PyMuPDFReader()
# docs0 = loader.load_data(file=Path("./data/llama2.pdf"))
docs = loader.load(file_path="Chapter-2-Introduction-to-the-Indian-FS-and-Markets.pdf")

from llama_index.core import Document
doc_text = "\n\n".join([d.get_content() for d in docs])
docs = [Document(text=doc_text)]

"""### creating heirarchies"""

from llama_index.core.node_parser import HierarchicalNodeParser,get_leaf_nodes

node_parser = HierarchicalNodeParser.from_defaults()

nodes = node_parser.get_nodes_from_documents(docs)
leaf_nodes = get_leaf_nodes(nodes)

nodes_by_id = {node.node_id: node for node in nodes}

"""### creating datset for elastic search"""

parent_ids_list = []
for i in range(0,len(leaf_nodes)):
    parent_ids_list.append(leaf_nodes[i].parent_node.node_id)

child_ids_list = []
for i in range(0,len(leaf_nodes)):
    child_ids_list.append(leaf_nodes[i].node_id)

child_contexts_list = []
for i in range(0,len(leaf_nodes)):
    child_contexts_list.append(leaf_nodes[i].text)

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-mpnet-base-v2')

import pandas as pd

df = pd.DataFrame({'parent_id':parent_ids_list,'child_id':child_ids_list,'child_context':child_contexts_list})

df["vectors"] = df["child_context"].apply(lambda x: model.encode(x))

df

"""### adding up data in elastic search"""

indexMapping = {
    "properties":{
        "parent_id":{
            "type":"text"
        },
        "child_id":{
            "type":"text"
        },
        "child_context":{
            "type":"text"
        },
        "vectors":{
            "type":"dense_vector",
            "dims":768,
            "index":True,
            "similarity":"l2_norm"
        }
    }
}

from elasticsearch import Elasticsearch

es = Elasticsearch(
    "https://localhost:9200",
    basic_auth=("elastic","=PGMBBMeamIbyzgpOeTB"),
    ca_certs="C:/Users/VANSH KHANEJA/PROJECTS/superteams_projects/ELASTIC SEARCH RAG/elasticsearch-8.14.1/config/certs/http_ca.crt"
)
es.ping()

es.ping()

es.indices.create(index="finance",mappings= indexMapping)

record_list = df.to_dict("records")
record_list

for record in record_list:
    try:
        es.index(index="finance",document=record)
    except Exception as e:
        print(e)

es.count(index="finance")

"""### fetching reults"""

test_query = "What are the Features of Capital Markets ?"
def find_matching_parent_ids(input_query):
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-mpnet-base-v2')
    vector_of_query = model.encode(input_query)
    query = {
            "field":"vectors",
            "query_vector":vector_of_query,
            "k":25,
            "num_candidates":299,
        }
    results = es.knn_search(index="finance",
                            knn=query,
                            source=["child_context","parent_id"]
                            )
    fetched_ids = []


    for i in results["hits"]["hits"]:
        fetched_ids.append(i['_source']['parent_id'])
    return fetched_ids


fetched_parent_ids = find_matching_parent_ids(test_query)
fetched_parent_ids

"""### finding the most common chunks"""

def most_frequent_parent_ids(list_of_id):
    frequency_dict = {}
    threshold = 5
    for element in list_of_id:
        if element in frequency_dict:
            frequency_dict[element] += 1
        else:
            frequency_dict[element] = 1


    sorted_elements = sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True)
    most_common_ids = []
    for i in range(0,threshold):
        most_common_ids.append(sorted_elements[i][0])
    return most_common_ids


most_common_ids=most_frequent_parent_ids(fetched_parent_ids)
most_common_ids

"""### GETTING THEIR PARENT context"""

parent_context_list = []

for y in most_common_ids:
    for i in range(0,len(leaf_nodes)):
        if(y== leaf_nodes[i].parent_node.node_id):
            parent_context_list.append(nodes_by_id[leaf_nodes[i].parent_node.node_id].text)
            break

"""### performing reranking over it"""

from sentence_transformers import CrossEncoder
rankmodel = CrossEncoder("jinaai/jina-reranker-v1-tiny-en")

query = test_query
results = rankmodel.rank(query, parent_context_list, return_documents=True, top_k=5)

reranked_list = []
for i in range(0,5):
    reranked_list.append(results[i]['corpus_id'])

"""### collecting the most similar data"""

context = ""

for i in reranked_list[0:3]:
    context+=parent_context_list[i]+"\n\n\n\n\n\n"

print(context)

from langchain_groq import ChatGroq

llm = ChatGroq(temperature=0, model_name="llama3-70b-8192",groq_api_key="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")

"""### prompting"""

from langchain import PromptTemplate
from langchain import LLMChain

prompt_template = PromptTemplate(
    template="These are few Context: {context} for this question Question: {question} base on this context genrate a relevant concise Answer from thi context:",
    input_variables=["context", "question"]
)

llm_chain = LLMChain(llm=llm, prompt=prompt_template)

def generate_answer(context, question):
    input_data = {
        "context": context,
        "question": question
    }
    answer = llm_chain(input_data)
    return answer

"""## output"""

from rich import print
print("Your Question:  \n"+test_query+"\n\n"+"Bot Reply:  \n"+generate_answer(context, test_query)['text'])



while True:
    test_query = input("\n\n\n\nEnter your query: ")
    if test_query == "":
      break
    fetched_parent_ids = find_matching_parent_ids(test_query)
    most_common_ids=most_frequent_parent_ids(fetched_parent_ids)
    parent_context_list = []
    for y in most_common_ids:
        for i in range(0,len(leaf_nodes)):
            if(y== leaf_nodes[i].parent_node.node_id):
                parent_context_list.append(nodes_by_id[leaf_nodes[i].parent_node.node_id].text)
                break

    query = test_query
    results = rankmodel.rank(query, parent_context_list, return_documents=True, top_k=5)

    reranked_list = []
    for i in range(0,5):
        reranked_list.append(results[i]['corpus_id'])

    context = ""

    for i in reranked_list[0:3]:
        context+=parent_context_list[i]+"\n\n"

    print("Your Question:  \n"+test_query+"\n\n"+"Bot Reply:  \n"+generate_answer(context, test_query)['text'])



